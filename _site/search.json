[
  {
    "objectID": "posts/what-is-BLAS/2023-03-30-what-is-BLAS.html",
    "href": "posts/what-is-BLAS/2023-03-30-what-is-BLAS.html",
    "title": "What is BLAS?",
    "section": "",
    "text": "Basic Linear Algebra Subprograms(BLAS) is a set of low-level routines for linear algebra operations. Most of the numerical libraries like NumPy and R do their linear algebra computations based on these standard operations. BLAS is often optimized for specific hardware for better performance; thus, BLAS has numerous different implementations depending on the vendor or type of processing unit(CPU/GPU).\nOperations in BLAS are categorized into three levels based on their complexity.\n\nLevel 1: Vector operations\n\\(O(n)\\) operations on \\(O(n)\\) operands\nex)  axpy, “a x plus y” \\[\ny \\leftarrow \\alpha x + y\n\\]\nwhere \\(x, y \\in \\mathbb{R}^n\\) and \\(\\alpha \\in \\mathbb{R}\\). \\(x, y\\) are \\(n\\)-dimensional vectors each, hence we have \\(2n\\) numbers. \\(\\alpha\\) is a scalar, hence we have \\(2n + 1\\) numbers in total. \\[\ny_i = \\alpha \\times x_i + y_i \\quad \\text{for }i = 1, ..., n\n\\] We have two floating point operations(\\(1\\) multiply and \\(1\\) add) for each \\(i\\) and we have to repeat it for \\(n\\) times, thus we have \\(2n\\) floating point operations in total.\nIn summary,\n\\(2n + 1\\) numbers \\(\\rightarrow O(n)\\) operands\n\\(2n\\) floating point operations \\(\\rightarrow O(n)\\) operations\n\n\nLevel 2: Matrix-vector operations\n\\(O(n^2)\\) operations on \\(O(n^2)\\) operands\nex) gemv, “generalized matrix-vector multiplication” \\[\ny \\leftarrow \\alpha A x + \\beta y\n\\] where \\(A \\in \\mathbb{R}^{n \\times n}\\), \\(x, y \\in \\mathbb{R}^n\\) and \\(\\alpha,\\beta \\in \\mathbb{R}\\).\n\\[\ny_i = \\alpha \\times \\sum_{j} A_{ij}\\times x_j + \\beta \\times y_i  \\quad\n\\text{for }i = 1, ..., n\n\\]\nHere, \\(A\\) has \\(n^2\\) numbers, \\(x, y\\) has \\(n\\) numbers each, thus we have \\(n^2 + 2n + 2\\) numbers.\nFor each \\(i\\), we have \\(n+2\\) multiplies and \\((n-1) + 1\\) adds, hence \\(n(2n+2) = 2n^2 + 2n\\) FP_OPs.\nIn summary,\n\\(n^2 + 2n + 2\\) numbers \\(\\rightarrow O(n^2)\\) operands\n\\(2n^2 + 2n\\) floating point operations \\(\\rightarrow O(n^2)\\) operations\n\n\nLevel 3 Matrix-matrix operations\n\\(O(n^3)\\) operations on \\(O(n^2)\\) operands\nex) gemm, “generalized matrix multiplication” \\[\nC \\leftarrow \\alpha AB + \\beta C\n\\]\nwhere \\(A,B,C \\in \\mathbb{R}^{n \\times n}\\), and \\(\\alpha,\\beta \\in \\mathbb{R}\\).\n\\[\nC_{ij} = \\alpha \\times \\sum_{k} A_{ik}\\times B_{kj} + \\beta \\times C_{ij}  \\quad\n\\text{for }i,j = 1, ..., n\n\\] Here, \\(A,B,C\\) has \\(n^2\\) numbers each, thus we have \\(3n^2+2\\) numbers.\nFor each \\(i\\) and \\(j\\), we have \\(n+2\\) multiplies and \\((n-1) + 1\\) adds, hence \\(n^2(2n+2) = 2n^3+2n^2\\) FP_OPs.\nIn summary,\n\\(3n^2 + 2\\) numbers \\(\\rightarrow O(n^2)\\) operands\n\\(2n^3 + 2n^2\\) floating point operations \\(\\rightarrow O(n^3)\\) operations"
  },
  {
    "objectID": "posts/cond-mvnormal/2023-05-19-cond-mvnormal.html",
    "href": "posts/cond-mvnormal/2023-05-19-cond-mvnormal.html",
    "title": "Deriving conditional multivariate normal distribution",
    "section": "",
    "text": "Let random vector \\(\\mathbf{x}\\) follows a multivariate normal distribution \\(\\mathbf{x} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\). Consider the conditional distribution of \\(\\mathbf{x_1}\\vert\\mathbf{x_2}\\) where \\(\\mathbf{x_1}\\) and \\(\\mathbf{x_2}\\) are partitions of the random vector \\(\\mathbf{x}\\). We first create partitions as follows.\n\\[\n\\mathbf{x} = \\begin{bmatrix}\n\\mathbf{x_1}\\\\\n\\mathbf{x_2}\n\\end{bmatrix}\n\\qquad\n\\boldsymbol{\\mu} = \\begin{bmatrix}\n\\mathbf{\\mu_1}\\\\\n\\mathbf{\\mu_2}\n\\end{bmatrix}\n\\qquad\n\\boldsymbol{\\Sigma}=\n\\begin{bmatrix}\n\\Sigma_{11} &\\Sigma_{12} \\\\\n\\Sigma_{21} &\\Sigma_{22}\n\\end{bmatrix}\n\\]\nThen, \\(\\mathbf{x_1} \\vert \\mathbf{x_2} \\sim N(\\tilde{\\boldsymbol{\\mu}}, \\tilde{\\boldsymbol{\\Sigma}})\\) where \\(\\tilde{\\boldsymbol{\\mu}}\\) and \\(\\tilde{\\boldsymbol{\\Sigma}}\\) is given by,\n\\[\n\\tilde{\\boldsymbol{\\mu}} = \\mu_1 + \\Sigma_{12}\\Sigma_{22}^{-1}(\\mathbf{x_2} -\n\\boldsymbol{\\mu}_2) \\\\\n\\] \\[\n\\tilde{\\boldsymbol{\\boldsymbol{\\Sigma}}} = \\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\n\\]\n\nProof\nIt is known that conditional distribution of multivaritate normal, conditioned on a subset of variables are again multivariate normal. Thus, we can verify the above theorem by deriving the conditional mean and covariace. Assuming \\(\\Sigma_{22}\\) is invertible, define a new variable \\(\\mathbf{z} = \\mathbf{x_1} + A\\mathbf{x_2}\\), where \\(A = -\\Sigma_{12}\\Sigma_{22}^{-1}\\).\nFirst notice that \\(\\mathbf{z}\\) and \\(\\mathbf{x_2}\\) has zero covariance, \\[\n\\newcommand{\\cov}{\\text{Cov}}\n\\newcommand{\\var}{\\text{Var}}\n\\newcommand{\\A}{-\\Sigma_{12}\\Sigma_{22}^{-1}}\n\\newcommand{\\bv}[1]{\\mathbf{#1}}\n\\begin{aligned}\n\\cov(\\bv{z},\\ \\bv{x}_2) &= \\cov(\\bv{x}_1 + A\\bv{x}_2,\\ \\bv{x}_2) \\\\\n&= \\cov(\\bv{x}_1,\\ \\bv{x}_2) + \\cov(A\\bv{x}_2, \\bv{x}_2) \\\\\n&= \\cov(\\bv{x}_1,\\ \\bv{x}_2) + A\\cov(\\bv{x}_2, \\bv{x}_2) \\\\\n&= \\Sigma_{12} + A\\Sigma_{22} \\\\\n&= \\Sigma_{12} + \\A\\Sigma_{22} \\\\\n&= \\Sigma_{12} - \\Sigma_{12} \\\\\n&= 0 \\\\\n\\end{aligned}\n\\]\nSince \\(\\mathbf{z}\\) and \\(\\mathbf{x}\\) are jointly normal and uncorrelated, they are independent by the property of mulitvariate normal. Now we can easily find the conditional mean as follows.\n\\[\n\\newcommand{\\cov}{\\text{Cov}}\n\\newcommand{\\var}{\\text{Var}}\n\\newcommand{\\A}{-\\Sigma_{12}\\Sigma_{22}^{-1}}\n\\newcommand{\\bv}[1]{\\mathbf{#1}}\n\\newcommand{\\ind}{\\perp \\!\\!\\! \\perp}\n\\begin{aligned}\nE(\\bv{x}_1 \\vert \\bv{x}_2) &= E(\\bv{z} - A\\bv{x}_2 \\vert \\bv{x}_2) \\\\\n&= E(\\bv{z}\\vert \\bv{x}_2) - AE(\\bv{x}_2 \\vert \\bv{x}_2) \\\\\n&= E(\\bv{z}) - A\\bv{x}_2 \\qquad (\\because \\bv{x}_2 \\ind \\bv{z}) \\\\\n&= E(\\bv{x}_1 + A\\bv{x}_2) - A\\bv{x}_2 \\\\\n&= E(\\bv{x}_1) + AE(\\bv{x}_2) - A\\bv{x}_2 \\\\\n&= \\bv{\\mu}_1 + A\\bv{\\mu}_2 - A\\bv{x}_2 \\\\\n&= \\bv{\\mu}_1 - A(\\bv{x}_2 - \\bv{\\mu}_2)\\\\\n&= \\bv{\\mu}_1 + \\Sigma_{12}\\Sigma_{22}^{-1}(\\bv{x}_2 - \\bv{\\mu}_2)\\\\\n&= \\tilde{\\boldsymbol{\\mu}}\\\\\n\\end{aligned}\n\\]\nBefore jumping into conditional variance. Recall the following facts for random vectors \\(\\mathbf{x},\\ \\mathbf{y}\\) and non-random matrix \\(A, B\\),\n\\[\n\\newcommand{\\cov}{\\text{Cov}}\n\\newcommand{\\var}{\\text{Var}}\n\\newcommand{\\bv}[1]{\\mathbf{#1}}\n\\begin{aligned}\n\\var(\\bv{x} + \\bv{y}) &= \\var(\\bv{x}) + \\var(\\bv{y}) + \\cov(\\bv{x},\\ \\bv{y}) +\n\\cov(\\bv{y},\\ \\bv{x}) \\\\\n\\cov(A\\bv{x}, B\\bv{y}) &= A\\cov(\\bv{x}, \\bv{y})B^T\n\\end{aligned}\n\\]\nUsing the above fact, we can expand the conditional variance as follows.\n\\[\n\\newcommand{\\cov}{\\text{Cov}}\n\\newcommand{\\var}{\\text{Var}}\n\\newcommand{\\A}{-\\Sigma_{12}\\Sigma_{22}^{-1}}\n\\newcommand{\\bv}[1]{\\mathbf{#1}}\n\\newcommand{\\ind}{\\perp \\!\\!\\! \\perp}\n\\begin{aligned}\n\\var(\\bv{x}_1 \\vert \\bv{x}_2) &= \\var(\\bv{z} - A\\bv{x}_2 \\vert \\bv{x}_2) \\\\\n&= \\var(\\bv{z} \\vert \\bv{x}_2) + \\var(- A\\bv{x}_2 \\vert \\bv{x}_2) + \\cov(\\bv{z}\n,\\ - A\\bv{x}_2 \\vert \\bv{x}_2) + \\cov(- A\\bv{x}_2,\\ \\bv{z} \\vert \\bv{x}_2) \\\\\n\\end{aligned}\n\\]\nSince \\(-A\\mathbf{x}_2 \\vert \\mathbf{x}_2\\) is not random anymore given \\(\\mathbf{x}_2\\), we are only left with the first term.\n\\[\n\\newcommand{\\cov}{\\text{Cov}}\n\\newcommand{\\var}{\\text{Var}}\n\\newcommand{\\A}{\\Sigma_{12}\\Sigma_{22}^{-1}}\n\\newcommand{\\bv}[1]{\\mathbf{#1}}\n\\newcommand{\\ind}{\\perp \\!\\!\\! \\perp}\n\\begin{aligned}\n&= \\var(\\bv{z} \\vert \\bv{x}_2) \\\\\n&= \\var(\\bv{z})  \\qquad (\\because \\bv{x}_2 \\ind \\bv{z}) \\\\\n&= \\var(\\bv{x}_1 + A \\bv{x}_2)\\\\\n&= \\var(\\bv{x}_1) + \\var(A \\bv{x}_2) + \\cov(\\bv{x}_1,\\ A \\bv{x}_2) + \\cov(A \\bv{x}_2,\\ \\bv{x}_1)\\\\\n&= \\var(\\bv{x}_1) + A\\var(\\bv{x}_2)A^T + \\cov(\\bv{x}_1,\\ \\bv{x}_2)A^T + A\\cov(\\bv{x}_2,\\ \\bv{x}_1)\\\\\n&= \\Sigma_{11} + A\\Sigma_{22}A^T + \\Sigma_{12}A^T + A\\Sigma_{21}\\\\\n&= \\Sigma_{11} - A\\Sigma_{22} (\\A)^T + A\\Sigma_{21} + A\\Sigma_{21} \\\\\n&= \\Sigma_{11} - A\\Sigma_{22} \\Sigma_{22}^{-1}\\Sigma_{21} + 2A\\Sigma_{21} \\\\\n&= \\Sigma_{11} - A\\Sigma_{21} + 2A\\Sigma_{21} \\\\\n&= \\Sigma_{11} + A\\Sigma_{21} \\\\\n&= \\Sigma_{11} - \\A \\Sigma_{21} \\\\\n&= \\tilde{\\boldsymbol{\\Sigma}} \\\\\n\\end{aligned}\n\\]\nTherefore, the proof is done.\n\n\nReferences\n\n\n(https://stats.stackexchange.com/users/4856/macro), Macro. n.d. “Deriving the Conditional Distributions of a Multivariate Normal Distribution.” Cross Validated. https://stats.stackexchange.com/q/30600."
  },
  {
    "objectID": "posts/arbitrage/2022-11-4-arbitrage.html",
    "href": "posts/arbitrage/2022-11-4-arbitrage.html",
    "title": "Arbitrage",
    "section": "",
    "text": "What is arbitrage?\nWe say there’s an arbitrage if we can take some positive return without taking any risk by exploiting some market condition. We can define this notion in mathematical expression as follows.\n\nDefinition (Arbitrage)\nFor a portfolio that has value of \\(X_t\\) at time \\(t \\geq 0\\), there’s an arbitrage if \\(\\exists T &gt; 0\\) such that the following three conditions hold,\n\\[\n\\begin{aligned}\n1.\\quad &X_0 = 0\\\\\n2.\\quad &X_T \\geq 0 \\quad \\text{a.s.} \\\\\n3.\\quad &P(X_T &gt; 0) &gt; 0\\\\\n\\end{aligned}\n\\]\nThe first condition indicates the portfolio does not require any initial investment. The second condition states that an event of the terminal value is non-negative happens with probability 1, i.e., \\(P(X_T \\geq 0) = 1\\). The third condition implies that the probability of the terminal portfolio value being positive is greater than 0.\nThus, if there is a portfolio that requires zero initial investment but the terminal value will never be less than 0 and there’s a chance to have a positive return, then we say there’s an arbitrage opportunity.\n\n\nDefinition (Equivalence of probability measures)\nA probability measure \\(\\mathbb{\\tilde{P}}\\) is said to be equivalent to some other probability measure \\(\\mathbb{P}\\), and denoted \\(\\mathbb{P} \\sim \\mathbb{\\tilde{P}}\\) if they agree with the events that occur with probability 1. i.e.,\n\\[\n\\mathbb{P}(E) = 1 \\iff \\Bbb{\\tilde{P}}(E) = 1\n\\]\nWith the above definition of arbitrage and the notion of equivalent probability measure, we can derive some useful theorem as follows.\n\n\nTheorem\nIf \\(X\\) is arbitrage under some probability measure \\(\\mathbb{P}\\), then it is also arbitrage for any equivalent measure \\(\\mathbb{\\tilde{P}}\\).\n\nProof\nAssume that \\(\\mathbb{P} \\sim \\mathbb{\\tilde{P}}\\) and \\(X\\) is arbitrage under \\(\\mathbb{P}\\) but not arbitrage under \\(\\mathbb{\\tilde{P}}\\).\nIf X is an arbitrage under \\(\\mathbb{P}\\), by the second condition of arbitrage, \\[X_T \\geq 0 \\quad \\text{a.s.} \\iff \\Bbb{P}(X_T \\geq 0) = 1\\]\nSince equivalent probability measure \\(\\tilde{\\mathbb{P}}\\) agrees with the event that occurs with probability 1, we have, \\[\n\\mathbb{P}(X_T \\geq 0) = 1 \\iff \\tilde{\\mathbb{P}}(X_T \\geq 0) = 1\n\\]\nSince we know \\(\\mathbb{\\tilde{P}}(X_T \\geq 0) = 1\\), X is not an arbitrage if and only if when X violates the third condition under \\(\\mathbb{\\tilde{P}}\\). That is, \\(\\mathbb{\\tilde{P}}(X_T &gt; 0) = 0 \\iff \\mathbb{\\tilde{P}}(X_T = 0) = 1\\).\nHowever, the equivalent probability measures should agree on all events that occur with probability 1.\nSince we have \\(\\mathbb{P}(X_T &gt; 0) &gt; 0 \\iff \\mathbb{P}(X_T = 0) \\neq 1\\), there is a contradiction.\n\n\n\n\nReferences\n\n\nLépinette, Emmanuel. 2019. “A short introduction to arbitrage theory and pricing in mathematical finance for discrete-time markets with or without friction.” Master. France. https://hal.archives-ouvertes.fr/cel-02125685.\n\n\nLorig, Matt. 2022. “CFRM 504: Lecture Notes.” Department of Applied Mathematics, University of Washington."
  },
  {
    "objectID": "posts/Unbiased-sample-variance/2023-01-14-Unbiased sample variance.html",
    "href": "posts/Unbiased-sample-variance/2023-01-14-Unbiased sample variance.html",
    "title": "Unbiased Sample Variance",
    "section": "",
    "text": "The definition of Variance of a random variable X is given by \\[\nVar[X] = E\\left[(X_i - E[X])^2\\right]\n\\]\nFor the sample estimator of the variance, we often use the following sample variance formula.\n\\[\n\\hat{\\sigma}^2 = \\frac{1}{n-1}\\left(\\sum_{i=1}^n (X_i - \\bar{X})^2\\right)\n\\]\nWhy do we divide by \\((n-1)\\) instead of \\(n\\)? This might be confusing since we don’t divide by \\((n-1)\\) for the sample mean \\(\\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n}\\).\n\n\nIt’s because the sample variance \\(\\hat{\\sigma}^2\\) with \\((n-1)\\) denominator is an unbiased estimator of the true variance \\(\\sigma^2\\) assuming that the samples are independent and identically distributed(iid).\n\nThis behavior is called Bessel’s correction. One thing you should keep in mind is that the Bessel’s correction is only for the samples from iid distribuiton. If the samples are not iid, (e.g. Time-Series data) the Bessel’s correction would not yield an unbiased estimator.\n\n\n\n\n\n\nWhen we estimate some unknown parameter \\(\\theta\\), we call an estimator \\(\\hat{\\theta}\\) is unbiased when \\(E[\\hat{\\theta}] = \\theta\\). We may think of an estimator \\(\\hat{\\theta}\\) as an educated guess for an unknown parameter \\(\\theta\\) under the available finite sample data \\((X_1, X_2, \\cdots, X_n)\\). Unbiasedness is one of the desirable properties for an estimator.\n\n\n\nLet \\(X_i\\)’s \\((i = 1, 2, \\cdots, n)\\) are iid random samples, from a distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) , and let \\(\\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n}\\).\n\\[\n\\begin{aligned}\n\\hat{\\sigma}^2 &= \\frac{1}{n-1}\\left(\\sum_{i=1}^n (X_i - \\bar{X})^2\\right) \\\\\n&= \\frac{1}{n-1}\\left(\\sum_{i=1}^n (X_i^2 - 2X_i\\bar{X} + \\bar{X}^2)\\right) \\\\\n&= \\frac{1}{n-1}\\left(\\sum_{i=1}^n X_i^2 - \\sum_{i=1}^n 2X_i\\bar{X} + \\sum_{i=1}^n \\bar{X}^2)\\right) \\\\\n&= \\frac{1}{n-1}\\left(\\sum_{i=1}^n X_i^2 - 2\\bar{X}\\sum_{i=1}^n X_i + n \\bar{X}^2)\\right) \\\\\n&= \\frac{1}{n-1}\\left(\\sum_{i=1}^n X_i^2 - 2\\bar{X}n\\bar{X} + n \\bar{X}^2)\\right) \\\\\n&= \\frac{1}{n-1}\\left(\\sum_{i=1}^n X_i^2 - 2n\\bar{X}^2 + n \\bar{X}^2)\\right) \\\\\n&= \\frac{1}{n-1}\\left(\\sum_{i=1}^n X_i^2 - n\\bar{X}^2\\right) \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nE[\\hat{\\sigma}^2] &= E\\left[\\frac{1}{n-1}\\left(\\sum_{i=1}^n X_i^2 - n\\bar{X}^2\\right)\\right] \\\\\n&= \\frac{1}{n-1}E\\left[\\left(\\sum_{i=1}^n X_i^2 - n\\bar{X}^2\\right)\\right] \\\\\n&= \\frac{1}{n-1}\\left(E\\left[\\sum_{i=1}^n X_i^2 \\right] - E\\left[ n\\bar{X}^2\\right]\\right) \\\\\n&= \\frac{1}{n-1}\\left(\\sum_{i=1}^n E\\left[ X_i^2 \\right] - nE\\left[ \\bar{X}^2\\right]\\right) \\qquad (1)\\\\\n\\end{aligned}\n\\]\nNote that \\[\n\\begin{aligned}\n\\sigma^2 &= E[X^2] - E[X]^2 \\\\\n&= E[X^2] - \\mu^2 \\\\\n\\\\\n\\therefore \\quad E[X^2] &= \\sigma^2 + \\mu^2 \\qquad (2)\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nVar[\\bar{X}] &= E[\\bar{X}^2] - E[\\bar{X}]^2 \\\\\nE[\\bar{X}^2] &= Var[\\bar{X}] + E[\\bar{X}]^2 \\\\\n\\end{aligned}\n\\] i) \\[\n\\begin{aligned}\nVar[\\bar{X}] &= Var\\left[ \\frac{\\sum X_i}{n} \\right] \\\\\n&= \\frac{1}{n^2} Var\\left[ \\sum X_i \\right] \\\\\n&= \\frac{1}{n^2} \\sum Var\\left[X_i \\right] \\quad \\because X_i\\text{'s are mutually ind.}\\\\\n&= \\frac{1}{n^2} n\\sigma^2 \\quad \\because X_i\\text{'s are identically distributed}\\\\\n&=\\frac{\\sigma^2}{n}\n\\end{aligned}\n\\] ii)\n\\[\n\\begin{aligned}\nE[\\bar{X}] &= E\\left[{\\frac{\\sum X_i}{n}}\\right] \\\\\n&= \\frac{1}{n}{\\sum E[X_i]} \\\\\n&= \\frac{1}{n}n\\mu \\\\\n&= \\mu \\\\\n\\end{aligned}\n\\]\n\\[\\therefore \\quad E[\\bar{X}^2] = Var[\\bar{X}] + E[\\bar{X}]^2 =\n\\frac{\\sigma^2}{n} - \\mu^2 \\qquad (3)\n\\] Thus, from (1), (2), (3),\n\\[\n\\begin{aligned}\nE[\\hat{\\sigma}^2] &= \\frac{1}{n-1}\\left(\\sum_{i=1}^n E\\left[ X_i^2 \\right] - nE\\left[ \\bar{X}^2\\right]\\right) \\\\\n&= \\frac{1}{n-1}\\left(\\sum_{i=1}^n (\\sigma^2 + \\mu^2) - n\\left(\\frac{\\sigma^2}{n} - \\mu^2\\right)\\right) \\\\\n&= \\frac{1}{n-1}\\left(n (\\sigma^2 + \\mu^2) - n\\left(\\frac{\\sigma^2}{n} - \\mu^2\\right)\\right) \\\\\n&= \\frac{1}{n-1}\\left(n\\sigma^2 + n\\mu^2 - \\sigma^2 - n\\mu^2\\right) \\\\\n&= \\frac{1}{n-1}(n - 1) \\sigma^2 \\\\\n&= \\sigma^2 \\\\\n\\end{aligned}\n\\]\n\\[\n\\therefore \\quad E[\\hat{\\sigma}^2] = \\sigma^2\n\\]\nSince the sample variance with the denominator of \\((n-1)\\) is an unbiased estimator for the true variance \\(\\sigma^2\\), any other estimator with different denominator would be biased.\n\n\n\nAlong the way to show the \\(\\hat{\\sigma}^2\\) is unbiased, we’ve already shown why we don’t apply the same logic for the sample mean \\(\\bar{X}\\). In ii), we derived that \\(\\bar{X}\\) is unbiased \\(E[\\bar{X}] = \\mu\\) without any correction, unlike the sample variance."
  },
  {
    "objectID": "posts/Unbiased-sample-variance/2023-01-14-Unbiased sample variance.html#why-do-we-divide-by-n-1-instead-of-n-when-we-calculate-the-sample-variance",
    "href": "posts/Unbiased-sample-variance/2023-01-14-Unbiased sample variance.html#why-do-we-divide-by-n-1-instead-of-n-when-we-calculate-the-sample-variance",
    "title": "Unbiased Sample Variance",
    "section": "",
    "text": "The definition of Variance of a random variable X is given by \\[\nVar[X] = E\\left[(X_i - E[X])^2\\right]\n\\]\nFor the sample estimator of the variance, we often use the following sample variance formula.\n\\[\n\\hat{\\sigma}^2 = \\frac{1}{n-1}\\left(\\sum_{i=1}^n (X_i - \\bar{X})^2\\right)\n\\]\nWhy do we divide by \\((n-1)\\) instead of \\(n\\)? This might be confusing since we don’t divide by \\((n-1)\\) for the sample mean \\(\\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n}\\).\n\n\nIt’s because the sample variance \\(\\hat{\\sigma}^2\\) with \\((n-1)\\) denominator is an unbiased estimator of the true variance \\(\\sigma^2\\) assuming that the samples are independent and identically distributed(iid).\n\nThis behavior is called Bessel’s correction. One thing you should keep in mind is that the Bessel’s correction is only for the samples from iid distribuiton. If the samples are not iid, (e.g. Time-Series data) the Bessel’s correction would not yield an unbiased estimator.\n\n\n\n\n\n\nWhen we estimate some unknown parameter \\(\\theta\\), we call an estimator \\(\\hat{\\theta}\\) is unbiased when \\(E[\\hat{\\theta}] = \\theta\\). We may think of an estimator \\(\\hat{\\theta}\\) as an educated guess for an unknown parameter \\(\\theta\\) under the available finite sample data \\((X_1, X_2, \\cdots, X_n)\\). Unbiasedness is one of the desirable properties for an estimator.\n\n\n\nLet \\(X_i\\)’s \\((i = 1, 2, \\cdots, n)\\) are iid random samples, from a distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) , and let \\(\\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n}\\).\n\\[\n\\begin{aligned}\n\\hat{\\sigma}^2 &= \\frac{1}{n-1}\\left(\\sum_{i=1}^n (X_i - \\bar{X})^2\\right) \\\\\n&= \\frac{1}{n-1}\\left(\\sum_{i=1}^n (X_i^2 - 2X_i\\bar{X} + \\bar{X}^2)\\right) \\\\\n&= \\frac{1}{n-1}\\left(\\sum_{i=1}^n X_i^2 - \\sum_{i=1}^n 2X_i\\bar{X} + \\sum_{i=1}^n \\bar{X}^2)\\right) \\\\\n&= \\frac{1}{n-1}\\left(\\sum_{i=1}^n X_i^2 - 2\\bar{X}\\sum_{i=1}^n X_i + n \\bar{X}^2)\\right) \\\\\n&= \\frac{1}{n-1}\\left(\\sum_{i=1}^n X_i^2 - 2\\bar{X}n\\bar{X} + n \\bar{X}^2)\\right) \\\\\n&= \\frac{1}{n-1}\\left(\\sum_{i=1}^n X_i^2 - 2n\\bar{X}^2 + n \\bar{X}^2)\\right) \\\\\n&= \\frac{1}{n-1}\\left(\\sum_{i=1}^n X_i^2 - n\\bar{X}^2\\right) \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nE[\\hat{\\sigma}^2] &= E\\left[\\frac{1}{n-1}\\left(\\sum_{i=1}^n X_i^2 - n\\bar{X}^2\\right)\\right] \\\\\n&= \\frac{1}{n-1}E\\left[\\left(\\sum_{i=1}^n X_i^2 - n\\bar{X}^2\\right)\\right] \\\\\n&= \\frac{1}{n-1}\\left(E\\left[\\sum_{i=1}^n X_i^2 \\right] - E\\left[ n\\bar{X}^2\\right]\\right) \\\\\n&= \\frac{1}{n-1}\\left(\\sum_{i=1}^n E\\left[ X_i^2 \\right] - nE\\left[ \\bar{X}^2\\right]\\right) \\qquad (1)\\\\\n\\end{aligned}\n\\]\nNote that \\[\n\\begin{aligned}\n\\sigma^2 &= E[X^2] - E[X]^2 \\\\\n&= E[X^2] - \\mu^2 \\\\\n\\\\\n\\therefore \\quad E[X^2] &= \\sigma^2 + \\mu^2 \\qquad (2)\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nVar[\\bar{X}] &= E[\\bar{X}^2] - E[\\bar{X}]^2 \\\\\nE[\\bar{X}^2] &= Var[\\bar{X}] + E[\\bar{X}]^2 \\\\\n\\end{aligned}\n\\] i) \\[\n\\begin{aligned}\nVar[\\bar{X}] &= Var\\left[ \\frac{\\sum X_i}{n} \\right] \\\\\n&= \\frac{1}{n^2} Var\\left[ \\sum X_i \\right] \\\\\n&= \\frac{1}{n^2} \\sum Var\\left[X_i \\right] \\quad \\because X_i\\text{'s are mutually ind.}\\\\\n&= \\frac{1}{n^2} n\\sigma^2 \\quad \\because X_i\\text{'s are identically distributed}\\\\\n&=\\frac{\\sigma^2}{n}\n\\end{aligned}\n\\] ii)\n\\[\n\\begin{aligned}\nE[\\bar{X}] &= E\\left[{\\frac{\\sum X_i}{n}}\\right] \\\\\n&= \\frac{1}{n}{\\sum E[X_i]} \\\\\n&= \\frac{1}{n}n\\mu \\\\\n&= \\mu \\\\\n\\end{aligned}\n\\]\n\\[\\therefore \\quad E[\\bar{X}^2] = Var[\\bar{X}] + E[\\bar{X}]^2 =\n\\frac{\\sigma^2}{n} - \\mu^2 \\qquad (3)\n\\] Thus, from (1), (2), (3),\n\\[\n\\begin{aligned}\nE[\\hat{\\sigma}^2] &= \\frac{1}{n-1}\\left(\\sum_{i=1}^n E\\left[ X_i^2 \\right] - nE\\left[ \\bar{X}^2\\right]\\right) \\\\\n&= \\frac{1}{n-1}\\left(\\sum_{i=1}^n (\\sigma^2 + \\mu^2) - n\\left(\\frac{\\sigma^2}{n} - \\mu^2\\right)\\right) \\\\\n&= \\frac{1}{n-1}\\left(n (\\sigma^2 + \\mu^2) - n\\left(\\frac{\\sigma^2}{n} - \\mu^2\\right)\\right) \\\\\n&= \\frac{1}{n-1}\\left(n\\sigma^2 + n\\mu^2 - \\sigma^2 - n\\mu^2\\right) \\\\\n&= \\frac{1}{n-1}(n - 1) \\sigma^2 \\\\\n&= \\sigma^2 \\\\\n\\end{aligned}\n\\]\n\\[\n\\therefore \\quad E[\\hat{\\sigma}^2] = \\sigma^2\n\\]\nSince the sample variance with the denominator of \\((n-1)\\) is an unbiased estimator for the true variance \\(\\sigma^2\\), any other estimator with different denominator would be biased.\n\n\n\nAlong the way to show the \\(\\hat{\\sigma}^2\\) is unbiased, we’ve already shown why we don’t apply the same logic for the sample mean \\(\\bar{X}\\). In ii), we derived that \\(\\bar{X}\\) is unbiased \\(E[\\bar{X}] = \\mu\\) without any correction, unlike the sample variance."
  },
  {
    "objectID": "posts/Unbiased-sample-variance/2023-01-14-Unbiased sample variance.html#implementations",
    "href": "posts/Unbiased-sample-variance/2023-01-14-Unbiased sample variance.html#implementations",
    "title": "Unbiased Sample Variance",
    "section": "Implementations",
    "text": "Implementations\nIn most cases, we don’t explicitly calculate the sample variance. Instead, we call the implemented function from the numerical library we use. Thus, it is useful to know whether the function uses Bessel’s correction or not, since it varies by the implementation details.\n\nNumpy\nNumpy has the np.var method to calculate the variance of a given input. np.var returns the uncorrected variance, which is divided by \\(n\\) unless you specify ddof argument. To get an unbiased sample variance, you have to add ddof=1. The np.var will return the result divided by n-ddof.\nimport numpy as np\nx = np.array([1,2,3])\nnp.var(x) # Divide by n where n = len(x)\nnp.var(x, ddof=1) # Divide by (n-1)\n\n\nPandas\nPandas has a method in its DataFrame class, pd.DataFrame.var. Unlike the np.var, pd.DataFrame.var returns the corrected sample variance with the denominator of \\((n-1)\\). This can also be changed by specifying the ddof argument as in np.var.\nimport pandas as pd\nx = pd.DataFrame([1,2,3])\nx.var() # Divide by (n-1)\nx.var(ddof=0) # Divide by n = (n-ddof)\n\n\nR\nStandard function var in R returns the unbiased sample variance with \\((n-1)\\) denominator. Unlike numpy and pandas, this behavior cannot be changed. Thus you should correct externally if you need to.\n\nThese functions use \\(n−1\\) on the denominator purely for consistency with stats::var() (for the record, I disagree with the rationale for \\(n−1\\)). -R Documentation-\n\nx &lt;- rnorm(10)\nn &lt;- len(x)\nvar(x) # Divide by (n-1)\nvar(x) * (n-1) / n # Divide by n"
  },
  {
    "objectID": "posts/Unbiased-sample-variance/2023-01-14-Unbiased sample variance.html#conclusion",
    "href": "posts/Unbiased-sample-variance/2023-01-14-Unbiased sample variance.html#conclusion",
    "title": "Unbiased Sample Variance",
    "section": "Conclusion",
    "text": "Conclusion\nNow we know why we divide by \\(n-1\\) instead of \\(n\\) for the sample variance. However, this doesn’t mean that you should always use Bessel’s correction. As I mentioned in the beginning, Bessel’s correction only works with the iid samples. And even for the iid samples, one might want to use a different estimator depending on the problem. Again, unbiasedness is just another good property to have. In fact, the uncorrected sample variance gives the Maximum Likelihood Estimator(MLE) when the samples are from iid normal. If one prefers the maximum likelihood over the unbiasedness, the uncorrected sample variance still gives a good estimation."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Eunki Chung\n\n\n\nThings Worth Remembering: My personal journal of interesting nuggets from the worlds of math, coding, and more. Simple things I stumble upon that I think are worth holding onto.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeriving conditional multivariate normal distribution\n\n\nConditional distribution when multivariate normal random vector is conditioned on its partition.\n\n\n\n\nstatistics\n\n\nmathematics\n\n\n \n\n\n\n\nMay 19, 2023\n\n\n\n\n\n\n\n\nImportance Sampling and Black Magic of Monte Carlo\n\n\nNotes on Importance Sampling and Exponential Smoothing\n\n\n\n\nmathematics\n\n\n \n\n\n\n\nApr 16, 2023\n\n\n\n\n\n\n\n\nWhat is BLAS?\n\n\nNotes on Basic Linear Algebra Subprograms\n\n\n\n\ncomputer science\n\n\nmathematics\n\n\n \n\n\n\n\nMar 30, 2023\n\n\n\n\n\n\n\n\nHow to add an email address on a pdf file rendered with Quarto\n\n\nCustomizing quarto document with a title.tex file\n\n\n\n\nquarto\n\n\n \n\n\n\n\nJan 30, 2023\n\n\n\n\n\n\n\n\nUnbiased Sample Variance\n\n\nBessel’s correction for the sample variance\n\n\n\n\nstatistics\n\n\n \n\n\n\n\nJan 14, 2023\n\n\n\n\n\n\n\n\nDeriving Black-Scholes Partial Differential Equation\n\n\nDerive Black-Scholes PDE with pricing by replication\n\n\n\n\nfinance\n\n\nmathematics\n\n\n \n\n\n\n\nNov 25, 2022\n\n\n\n\n\n\n\n\nArbitrage\n\n\nDefinition and theorem related to arbitrage\n\n\n\n\nfinance\n\n\nmathematics\n\n\n \n\n\n\n\nNov 4, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Eunki Chung\n\n\n\nI’m just a curious mind exploring the world of Computational Finance, always excited about topics like Machine Learning, Statistics, and DeFi (a fancy term for the future of finance). I spend lots of time reading about math and programming—my two loves.\nAnd when I need a break? You’ll find me bouldering, pushing my physical boundaries as much as my intellectual ones.\n\n\n\n\n\n\n\n\n   eunkich@uw.edu \n\n\n\n  Resume    LinkedIn    Github    Leetcode"
  },
  {
    "objectID": "posts/add-email-on-quarto/2023-01-30-add-email-on-quarto.html",
    "href": "posts/add-email-on-quarto/2023-01-30-add-email-on-quarto.html",
    "title": "How to add an email address on a pdf file rendered with Quarto",
    "section": "",
    "text": "By default, Quarto ignores a newline operator for an author name. So instead of adding an email address in the author variable, you need to customize the title part of the document as follows.\nConfigure the required information at the top of the jupyter notebook in a markdown format. The title.tex under the template-partials is the tex file that we’ll use for the title box of our document. The title.tex can be replaced with the path of the tex file, for instance ../path_to_title/title.tex.\n---\ntitle: \"Title of the Document\"\nsubtitle: \"Subtitle of the Document\"\nauthor: \"Your Name\"\nemail: \"yourid@email.com\"\ndate: today\ndate-format: long\noutput: pdf_document\nformat:\n  pdf:\n    template-partials:\n      - title.tex\n---\nCreate a title.tex file as follows,\n$if(title)$\n\\title{$title$$if(thanks)$\\thanks{$thanks$}$endif$}\n$endif$\n$if(subtitle)$\n$if(beamer)$\n$else$\n\\usepackage{etoolbox}\n\\makeatletter\n\\providecommand{\\subtitle}[1]{ % add subtitle to \\maketitle\n\\apptocmd{\\@title}{\\par {\\large #1 \\par}}{}{}\n}\n\\makeatother\n$endif$\n\\subtitle{$subtitle$}\n$endif$\n\\author{$author$ \\\\ \\small{$email$}}\n\\date{$date$}\nYou can further customize the title.tex for any other information you want to include. After the rendering to pdf, the result should look like this.\n\n\nReferences\nhttps://quarto.org/docs/authoring/title-blocks.html https://quarto.org/docs/journals/templates.html#template-partials"
  },
  {
    "objectID": "posts/black-scholes-pde/2022-11-25-black-scholes-pde.html",
    "href": "posts/black-scholes-pde/2022-11-25-black-scholes-pde.html",
    "title": "Deriving Black-Scholes Partial Differential Equation",
    "section": "",
    "text": "Black-Scholes PDE or Black-Scholes equation is an equation that governs the dynamics of European option value under the Black-Scholes model. The equation is given by,\n\\[\n\\frac{\\partial v}{\\partial t} + \\frac{1}{2}\\sigma^2 s^2\\frac{\\partial^2 v}{\\partial s^2} + r s \\frac{\\partial v}{\\partial s} - rv = 0\n\\]"
  },
  {
    "objectID": "posts/black-scholes-pde/2022-11-25-black-scholes-pde.html#black-scholes-partial-differential-equation",
    "href": "posts/black-scholes-pde/2022-11-25-black-scholes-pde.html#black-scholes-partial-differential-equation",
    "title": "Deriving Black-Scholes Partial Differential Equation",
    "section": "",
    "text": "Black-Scholes PDE or Black-Scholes equation is an equation that governs the dynamics of European option value under the Black-Scholes model. The equation is given by,\n\\[\n\\frac{\\partial v}{\\partial t} + \\frac{1}{2}\\sigma^2 s^2\\frac{\\partial^2 v}{\\partial s^2} + r s \\frac{\\partial v}{\\partial s} - rv = 0\n\\]"
  },
  {
    "objectID": "posts/black-scholes-pde/2022-11-25-black-scholes-pde.html#black-scholes-model",
    "href": "posts/black-scholes-pde/2022-11-25-black-scholes-pde.html#black-scholes-model",
    "title": "Deriving Black-Scholes Partial Differential Equation",
    "section": "Black-Scholes Model",
    "text": "Black-Scholes Model\n\\(dS_t = \\mu S_t dt + \\sigma S_t dW_t\\)\n\\(dB_t = rB_tdt\\)\nwhere \\((W_t)_{0\\leq t\\leq T}\\) is a Brownian motion.\n\n\nI will use the following known facts without proof.\nFor infinitesimally small \\(dt\\),\n\\(dW_t \\sim O(\\sqrt{dt}) = O(dt^{1/2})\\)\n\\((dW_t)^2 =dt\\)"
  },
  {
    "objectID": "posts/black-scholes-pde/2022-11-25-black-scholes-pde.html#pricing-by-replication",
    "href": "posts/black-scholes-pde/2022-11-25-black-scholes-pde.html#pricing-by-replication",
    "title": "Deriving Black-Scholes Partial Differential Equation",
    "section": "Pricing by replication",
    "text": "Pricing by replication\nConsider a European option with payoff \\(\\varphi(S_T)\\) at maturity \\(T\\) that has its value of \\(V_t\\) at time \\(t\\).\nConstruct a portfolio \\(X\\) that replicates the payoff of this European option. i.e., \\(X_t = V_t\\)\n\nLet \\(X_t = \\Delta_t S_t + \\frac{X_t - \\Delta_t S_t}{B_t}B_t\\).\nwhere \\(\\Delta_t\\) indicates the number of shares of stocks in the portfolio.\n\nIf the value of \\(X_t\\) only depends on the value of \\(S\\) and \\(B\\) then\n \\[\n\\begin{aligned}\ndX_t &= \\Delta_t dS_t + \\frac{X_t - \\Delta_t S_t}{B_t}dB_t \\\\\n&= \\Delta_t (\\mu S_tdt + \\sigma S_t dW_t) + \\frac{X_t - \\Delta_t S_t}{B_t}rB_tdt \\\\\n&= \\Delta_t (\\mu S_tdt + \\sigma S_t dW_t) + (X_t - \\Delta_t S_t)rdt \\\\\n&= \\Delta_tS_t (\\mu -r)dt + \\Delta_t S_t \\sigma dW_t + rX_tdt \\\\\n\\end{aligned}\n\\] \n\\[\nd\\left(\\frac{X_t}{B_t}\\right) = d\\left(X_t\\cdot \\frac{1}{B_t}\\right) = dX_t \\cdot \\frac{1}{B_t} + X_t \\cdot d\\left(\\frac{1}{B_t}\\right) + dX_t d\\left(\\frac{1}{B_t}\\right)\n\\] \nNote that by letting \\(f(B_t) = \\frac{1}{B_t}\\) , for an infinitesimally small increment \\(dt\\), we have\n\\[\n\\begin{aligned}\nd\\left( \\frac{1}{B_t}\\right) = df(B_t) &= f'(B_t)dB_t + \\frac{1}{2}f''(B_t)(dB_t)^2 \\\\\n&= -\\frac{1}{B_t^2} \\cdot dB_t + O(dt^2) \\\\\n&= -\\frac{rB_tdt}{B_t^2} \\\\\n&= -\\frac{r}{B_t}dt\n\\end{aligned}\n\\\\\n\\] \n\\[\n\\\\\n\\begin{aligned}\nd\\left(\\frac{X_t}{B_t}\\right) &= dX_t \\cdot \\frac{1}{B_t} + X_t \\cdot d\\left(\\frac{1}{B_t}\\right) + dX_t d\\left(\\frac{1}{B_t}\\right) \\\\\n&= \\frac{dX_t}{B_t} + X_t\\left(-\\frac{r}{B_t}dt\\right) + dX_t \\left(-\\frac{r}{B_t}dt\\right) \\\\\n&= \\frac{\\Delta_tS_t (\\mu -r)dt + \\Delta_t S_t \\sigma dW_t + rX_tdt }{B_t} - \\frac{rX_t}{B_t}dt + O(dt^2 + dt^{3/2}) \\\\\n&= \\frac{\\Delta_tS_t (\\mu -r)dt + \\Delta_t S_t \\sigma dW_t + rX_tdt }{B_t} - \\frac{rX_t}{B_t}dt \\\\\n&=\\frac{\\Delta_tS_t}{B_t}(\\mu-r)dt + \\frac{\\Delta_tS_t}{B_t}\\sigma dW_t\n\\end{aligned}\n\\]\n\nNow consider the value of the European option at time \\(t\\), \\(V_t.\\)\nIt is reasonable to assume that the value of a European option will only depend on time \\(t\\) and the stock price \\(S_t\\).\nThat is, assume \\(V_t\\)\\(=v(t, S_t)\\).\n By using the multi-dimensional Ito’s lemma,\n\\[\n\\begin{aligned}\ndV_t &= dv(t, S_t) \\\\\n&= \\frac{\\partial v}{\\partial t}dt + \\frac{\\partial v}{\\partial s} dS_t + \\frac{1}{2}\\frac{\\partial^2 v}{\\partial s^2}(dS_t)^2 \\\\\n&= \\frac{\\partial v}{\\partial t}dt + \\frac{\\partial v}{\\partial s} (\\mu S_t dt + \\sigma S_t dW_t) + \\frac{1}{2}\\frac{\\partial^2 v}{\\partial s^2}(\\mu S_t dt + \\sigma S_t dW_t)^2 \\\\\n&= \\frac{\\partial v}{\\partial t}dt + \\frac{\\partial v}{\\partial s} (\\mu S_t dt + \\sigma S_t dW_t) + \\frac{1}{2}\\frac{\\partial^2 v}{\\partial s^2}(\\mu^2 S_t^2 (dt)^2 + \\sigma^2 S_t^2 (dW_t)^2 + 2\\mu\\sigma S_t^2dtdW_t) \\\\\n&= \\frac{\\partial v}{\\partial t}dt + \\frac{\\partial v}{\\partial s} (\\mu S_t dt + \\sigma S_t dW_t) + \\frac{1}{2}\\frac{\\partial^2 v}{\\partial s^2}(O(dt^2) + \\sigma^2 S_t^2 dt + O(dt^{3/2})) \\\\\n&= \\frac{\\partial v}{\\partial t}dt + \\frac{\\partial v}{\\partial s} \\mu S_t dt + \\frac{\\partial v}{\\partial s}\\sigma S_t dW_t + \\frac{1}{2}\\frac{\\partial^2 v}{\\partial s^2}\\sigma^2 S_t^2 dt \\\\\n&= \\left[\\frac{\\partial v}{\\partial t} + \\frac{\\partial v}{\\partial s} \\mu S_t + \\frac{1}{2}\\frac{\\partial^2 v}{\\partial s^2}\\sigma^2 S_t^2 \\right] dt + \\frac{\\partial v}{\\partial s} \\sigma S_t dW_t \\\\\n\\end{aligned}\n\\] \n\\[\n\\begin{aligned}\nd\\left(\\frac{V_t}{B_t}\\right) &= d\\left(V_t\\cdot \\frac{1}{B_t}\\right) = dV_t \\cdot \\frac{1}{B_t} + V_t \\cdot d\\left(\\frac{1}{B_t}\\right) + dV_t d\\left(\\frac{1}{B_t}\\right) \\\\\n&= \\frac{dV_t}{B_t} + V_t \\left(-\\frac{r}{B_t}dt\\right) + dV_t d\\left(\\frac{1}{B_t}\\right) \\\\\n&= \\frac{dV_t}{B_t} - \\frac{rV_t}{B_t}dt + O(dt^2 +dt^{3/2}) \\\\\n&= \\frac{dV_t}{B_t} - \\frac{rV_t}{B_t}dt + O(dt^2 +dt^{3/2}) \\\\\n&= \\frac{1}{B_t}(dV_t - rV_tdt) \\\\\n&= \\frac{1}{B_t}\\left (\\left[\\frac{\\partial v}{\\partial t} + \\frac{\\partial v}{\\partial s} \\mu S_t + \\frac{1}{2}\\frac{\\partial^2 v}{\\partial s^2}\\sigma^2 S_t^2 \\right] dt + \\frac{\\partial v}{\\partial s} \\sigma S_t dW_t - rV_tdt \\right) \\\\\n&= \\frac{1}{B_t}\\left (\\left[\\frac{\\partial v}{\\partial t} + \\frac{\\partial v}{\\partial s} \\mu S_t + \\frac{1}{2}\\frac{\\partial^2 v}{\\partial s^2}\\sigma^2 S_t^2 - rV_t \\right] dt + \\frac{\\partial v}{\\partial s} \\sigma S_t dW_t \\right)\n\\end{aligned}\n\\] \nSuppose \\(X_0 = V_0\\) and \\(d\\left(\\frac{V_t}{B_t}\\right) = d\\left(\\frac{X_t}{B_t}\\right)\\), then we have\n\\(\\frac{X_t}{B_t} = \\frac{X_0}{B_0} + \\int_0^td\\left(\\frac{X_t}{B_t}\\right) = \\frac{V_0}{B_0} + \\int_0^td\\left(\\frac{V_t}{B_t}\\right) = \\frac{V_t}{B_t}\\).\n\nFrom \\(d\\left(\\frac{V_t}{B_t}\\right) = d\\left(\\frac{X_t}{B_t}\\right)\\),\n\\[\n\\begin{gather}\nd\\left(\\frac{V_t}{B_t}\\right) - d\\left(\\frac{X_t}{B_t}\\right) = 0\\\\\n\\frac{1}{B_t}\\left (\\left[\\frac{\\partial v}{\\partial t} + \\frac{\\partial v}{\\partial s} \\mu S_t + \\frac{1}{2}\\frac{\\partial^2 v}{\\partial s^2}\\sigma^2 S_t^2 - rV_t \\right] dt + \\frac{\\partial v}{\\partial s} \\sigma S_t dW_t \\right) - \\left( \\frac{\\Delta_tS_t}{B_t}(\\mu-r)dt + \\frac{\\Delta_tS_t}{B_t}\\sigma dW_t \\right)= 0 \\\\\n\\frac{1}{B_t}\\left (\\left[\\frac{\\partial v}{\\partial t} + \\frac{\\partial v}{\\partial s} \\mu S_t + \\frac{1}{2}\\frac{\\partial^2 v}{\\partial s^2}\\sigma^2 S_t^2 - rV_t - \\Delta_tS_t(\\mu-r)\\right] dt + \\left[\\frac{\\partial v}{\\partial s} \\sigma S_t - \\Delta_tS_t\\sigma  \\right] dW_t \\right) = 0 \\\\\n\\left[\\frac{\\partial v}{\\partial t} + \\frac{\\partial v}{\\partial s} \\mu S_t +\n\\frac{1}{2}\\frac{\\partial^2 v}{\\partial s^2}\\sigma^2 S_t^2 - rV_t -\n\\Delta_tS_t(\\mu-r)\\right] dt + \\left[\\frac{\\partial v}{\\partial s} - \\Delta_t\n\\right] \\sigma S_t dW_t = 0 \\\\\n\\end{gather}\n\\] \nChoose \\(\\Delta_t\\) s.t. \\(\\left[\\frac{\\partial v}{\\partial s} - \\Delta_t \\right] \\sigma S_t dW_t = 0\\), i.e., \\(\\Delta_t = \\frac{\\partial v}{\\partial s}\\).\n\\[\n\\begin{gather}\n\\left[\\frac{\\partial v}{\\partial t} + \\frac{\\partial v}{\\partial s} \\mu S_t + \\frac{1}{2}\\frac{\\partial^2 v}{\\partial s^2}\\sigma^2 S_t^2 - rV_t -\\frac{\\partial v}{\\partial s} S_t(\\mu-r)\\right] dt = 0 \\\\\n\\left[\\frac{\\partial v}{\\partial t} + \\frac{\\partial v}{\\partial s} \\mu S_t + \\frac{1}{2}\\frac{\\partial^2 v}{\\partial s^2}\\sigma^2 S_t^2 - rV_t -\\frac{\\partial v}{\\partial s} S_t\\mu + \\frac{\\partial v}{\\partial s} S_t r \\right] dt = 0 \\\\\n\\left[\\frac{\\partial v}{\\partial t} + \\frac{1}{2}\\frac{\\partial^2 v}{\\partial s^2}\\sigma^2 S_t^2 - rV_t + \\frac{\\partial v}{\\partial s} S_t r \\right] dt = 0 \\\\\n\\frac{\\partial v}{\\partial t} + \\frac{1}{2}\\frac{\\partial^2 v}{\\partial s^2}\\sigma^2 s^2 - rv + \\frac{\\partial v}{\\partial s} r s= 0\n\\end{gather}\n\\] \nBy rearranging the expression, we have the Black-Scholes Partial Differential Equation.\n\\[\n\\frac{\\partial v}{\\partial t} + \\frac{1}{2}\\sigma^2 s^2\\frac{\\partial^2 v}{\\partial s^2} + r s \\frac{\\partial v}{\\partial s} - rv = 0\n\\]"
  },
  {
    "objectID": "posts/monte-carlo/2023-04-16-monte-carlo.html",
    "href": "posts/monte-carlo/2023-04-16-monte-carlo.html",
    "title": "Importance Sampling and Black Magic of Monte Carlo",
    "section": "",
    "text": "The Monte Carlo method is a computational method that approximates the target value by repetitive random sampling. The goal of the method is to evaluate a function \\(g(X)\\) where \\(X\\) is some random quantity. One naive approach is generating random samples, \\(X_1, X_2, ... X_n\\), from computer simulation using pseudo-random number generation, and taking the average of evaluated \\(g(X_i)\\). That is, \\[\nE[g(X)] \\approx \\frac{1}{n} \\sum_{i=1}^{n} g(X_i)\n\\] This is called the direct sampling method and is indeed powerful in many settings. However, it has a severe shortcoming when the function is related to the occurrence of rare events. Let’s assume we have a standard normal random variable \\(X\\sim N(0, 1)\\) and want to evaluate a function \\(g(X) = \\mathbb{1}(X &gt; 30)\\). Note that \\(E[\\mathbb{1}(X &gt; 30)] = P(X &gt; 30)\\) so we know that the analytical solution of this problem is not zero. And also, notice that the direct estimation is essentially just counting the number of samples greater than 30 and calculating the frequency of that event.\nFortunately, we can easily generate standard normal samples using the Box-Muller algorithm, but it is almost impossible to get a sample greater than 30. Our direct MC estimate will indicate 0 until we get a single observation greater than 30, which will require a tremendous amount of samples unless we are very lucky. Even if we observe a desired sample with a meager chance, the variance of the estimation, hence the efficiency of the method, would still be poor."
  },
  {
    "objectID": "posts/monte-carlo/2023-04-16-monte-carlo.html#univariate-case",
    "href": "posts/monte-carlo/2023-04-16-monte-carlo.html#univariate-case",
    "title": "Importance Sampling and Black Magic of Monte Carlo",
    "section": "Univariate case",
    "text": "Univariate case\nOne way to address this issue is using the Importance Sampling method. With the following simple algebra, we can convert the estimation to one that is amazingly more efficient than the direct method. \\[\n\\begin{aligned}\nE[g(X)] &= \\int_{-\\infty}^{\\infty} g(x)f_X(x)dx \\\\\n&= \\int_{-\\infty}^{\\infty} g(x) \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right)dx \\\\\n&= \\int_{-\\infty}^{\\infty} g(x) \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}+\\mu x -\\frac{\\mu^2}{2} -\\mu x + \\frac{\\mu^2}{2}\\right)dx \\\\\n&= \\int_{-\\infty}^{\\infty} g(x) \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{(x - \\mu)^2}{2} -\\mu x + \\frac{\\mu^2}{2}\\right)dx \\\\\n&= \\int_{-\\infty}^{\\infty} g(x) \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{(x - \\mu)^2}{2}\\right) \\exp\\left( -\\mu x + \\frac{\\mu^2}{2}\\right)dx \\\\\n&= \\int_{-\\infty}^{\\infty} g(x) \\exp\\left( -\\mu x + \\frac{\\mu^2}{2}\\right) f_{X'}(x) dx \\quad \\text{where } X' \\sim N(\\mu, 1)\\\\\n&= E\\left[g(X') \\exp\\left( -\\mu X' + \\frac{\\mu^2}{2}\\right)\\right]\\\\\n&\\approx \\frac{1}{n}\\sum_{i=1}^n\\left\\{g(x'_i)\\exp\\left(-\\mu x'_i + \\frac{\\mu^2}{2}\\right)\\right\\}\n\\end{aligned}\n\\] Here, we’re sampling from a shifted normal variable \\(X'\\), evaluate \\(g\\) with samples \\(x_i'\\) and do a correction by multiplying \\(\\exp\\left(-\\mu x'_i + \\frac{\\mu^2}{2}\\right)\\) to retrieve the original target value. If we choose a \\(\\mu = 30\\), we can easily sample \\(x'&gt;30\\) and this significantly improves the quality of the Monte Carlo estimate."
  },
  {
    "objectID": "posts/monte-carlo/2023-04-16-monte-carlo.html#exponential-smoothing",
    "href": "posts/monte-carlo/2023-04-16-monte-carlo.html#exponential-smoothing",
    "title": "Importance Sampling and Black Magic of Monte Carlo",
    "section": "Exponential Smoothing",
    "text": "Exponential Smoothing\nIt turns out that there is a direct analog of this approach for the Multivariate case and even for the stochastic process, called Exponential Smoothing. For example, let’s consider the following Brownian Motion(BM) with drift. \\[\nX_t = B_t + \\mu t \\quad t \\geq 0\n\\] and we have discrete time points s.t. \\(0=t_0 &lt; t_1&lt; \\cdots &lt; t_n = T\\).\nSimilar to the Univariate case, our objective is to evaluate the expectation, \\(E[f(X_{t_1}, X_{t_2}, \\dots, X_{t_n})]\\), where \\(f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) is a bounded Borel function. Notice that since \\(B_t\\) is BM, \\(X_t\\) has independent increments. Let \\(x_0 = 0\\), and \\(g: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) s.t. \\(f(x_1, x_2, \\dots, x_n) = g(x_1 - x_0, x_2-x_1, \\cdots, x_n - x_{n-1})\\) Because of the independent increments, the joint density is a product of the marginal probability of the increments \\(X_i - X_{i-1}\\), which is given by. \\[\n\\begin{aligned}\nX_i - X_{i-1} &= B_{t_i} + \\mu t_i - B_{t_{i-1}} - \\mu t_{i-1} \\\\\n&= B_{t_i} - B_{t_{i-1}} + \\mu (t_i - t_{i-1}) \\\\\n\\end{aligned}\n\\] Since \\(B_{t_i} - B_{t_{i-1}} \\sim N(0,\\ t_i - t_{i-1})\\), \\(X_i - X_{i-1} \\sim N(\\mu(t_i - t_{i-1}),\\ t_i - t_{i-1})\\). Then the joint pdf is given by, \\[\n\\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}\\sqrt{t_i - t_{i-1}}} \\exp\\left(- \\frac{((x_i - x_{i-1}) - \\mu(t_i - t_{i-1}))^2}{2(t_i - t_{i-1})}\\right)\n\\] We can separate the constant term of \\[\nC = (2\\pi) ^{-n/2}\\cdot \\left( t_1-t_{0}\\right) ^{-1/2}\\cdot \\left( t_{2}-t_{1}\\right) ^{-1/2} \\ldots \\left( t_{n}-t_{n-1}\\right) ^{-1/2}\n\\] and the remaining exponent part, \\[\n\\begin{aligned}\n& \\prod_{i=1}^n \\exp\\left(- \\frac{((x_i - x_{i-1}) - \\mu(t_i - t_{i-1}))^2}{2(t_i - t_{i-1})}\\right)\\\\\n=& \\prod_{i=1}^n \\exp\\left(- \\frac{(x_i - x_{i-1})^2 -2(x_i - x_{i-1})\\mu(t_i - t_{i-1}) + \\mu^2(t_i - t_{i-1})^2}{2(t_i - t_{i-1})}\\right) \\\\\n=& \\prod_{i=1}^n \\exp\\left(- \\frac{(x_i - x_{i-1})^2}{2(t_i - t_{i-1})}  +(x_i - x_{i-1})\\mu - \\frac{\\mu^2}{2}(t_i - t_{i-1})\\right) \\\\\n=& \\prod_{i=1}^n \\exp\\left(- \\frac{(x_i - x_{i-1})^2}{2(t_i - t_{i-1})}\\right) \\exp\\left((x_i - x_{i-1})\\mu - \\frac{\\mu^2}{2}(t_i - t_{i-1})\\right) \\\\\n=& \\prod_{i=1}^n \\exp\\left(- \\frac{(x_i - x_{i-1})^2}{2(t_i - t_{i-1})}\\right) \\prod_{i=1}^n \\exp\\left((x_i - x_{i-1})\\mu - \\frac{\\mu^2}{2}(t_i - t_{i-1})\\right) \\\\\n=& \\left\\{\\prod_{i=1}^n \\exp\\left(- \\frac{(x_i - x_{i-1})^2}{2(t_i - t_{i-1})}\\right)\\right\\} \\exp\\left(\\sum_{i=1}^n(x_i - x_{i-1})\\mu - \\frac{\\mu^2}{2}(t_i - t_{i-1})\\right) \\\\\n=& \\left\\{\\prod_{i=1}^n \\exp\\left(- \\frac{(x_i - x_{i-1})^2}{2(t_i - t_{i-1})}\\right)\\right\\} \\exp\\left(\\mu\\sum_{i=1}^n (x_i - x_{i-1}) - \\frac{\\mu^2}{2} \\sum_{i=1}^n (t_i-t_{i-1})\\right) \\\\\n=& \\left\\{\\prod_{i=1}^n \\exp\\left(- \\frac{(x_i - x_{i-1})^2}{2(t_i - t_{i-1})}\\right)\\right\\} \\exp\\left(\\mu x_n - \\frac{1}{2}\\mu^2 t_n\\right)\n\\end{aligned}\n\\]\nNote that $C _{i=1}^n (- )$ is the joint density of \\((B_{t_1}- B_{t_0}, \\ B_{t_2}- B_{t_1}, \\cdots,\\ B_{t_n}-B_{t_{n-1}})\\). Then we can convert the original expectation as follows.\n\\[\nE[f(X_{t_1}, X_{t_2}, \\dots, X_{t_n})] = E\\left[f(B_{t_1}, B_{t_2}, \\dots, B_{t_n}) \\exp\\left(\\mu B_T - \\frac{1}{2}\\mu^2 T\\right)\\right]\n\\]\nAs in the Univariate case, we have converted the expection with respect to \\(B_t\\) instead of \\(X_t\\), evaluate \\(f(\\cdot)\\), then do a correction by multiplying \\(\\exp\\left(\\mu B_T - \\frac{1}{2}\\mu^2 T\\right)\\). One interesting fact is that the correction term \\(M_t = \\exp\\left(\\mu B_t - \\frac{1}{2}\\mu^2 t\\right)\\) is a martingale. \\[\n\\newcommand{\\indep}{\\perp \\!\\!\\! \\perp}\n\\begin{aligned}\nE[M_t|\\mathcal{F_s}] &= E\\left[\\exp\\left(\\left.\\mu B_t - \\frac{1}{2}\\mu^2 t\\right)\\right|\\mathcal{F_s}\\right] \\\\\n&= E\\left[\\exp\\left(\\left.\\mu B_t - \\mu B_s + \\mu B_s - \\frac{1}{2}\\mu^2 (t-s) - \\frac{1}{2}\\mu^2 s \\right)\\right|\\mathcal{F_s}\\right] \\\\\n&= E\\left[\\exp\\left(\\mu B_s - \\frac{1}{2}\\mu^2 s\\right) \\exp\\left(\\left.\\mu (B_t - B_s) - \\frac{1}{2}\\mu^2 (t-s)  \\right)\\right|\\mathcal{F_s}\\right] \\\\\n&= M_s E\\left[\\exp\\left(\\mu (B_t - B_s) - \\frac{1}{2}\\mu^2 (t-s)  \\right)\\right] \\quad \\because\\ B_t - B_s \\indep \\mathcal{F_s} \\\\\n&= M_s E\\left[\\exp\\left(\\mu (B_t - B_s)\\right) \\exp\\left(- \\frac{1}{2}\\mu^2 (t-s)  \\right)\\right] \\\\\n&= M_s \\exp\\left(- \\frac{1}{2}\\mu^2 (t-s)  \\right) E\\left[\\exp\\left(\\mu Y\\right) \\right] \\quad \\text{where } Y\\sim N(0,\\ t-s) \\\\\n\\\\\n\\text{Notice }&E\\left[\\exp\\left(\\mu Y\\right) \\right] \\text{ is the MGF of }Y \\\\\n\\\\\n&= M_s \\exp\\left(- \\frac{1}{2}\\mu^2 (t-s) \\right) \\exp\\left(\\frac{1}{2}\\mu^2 (t-s) \\right)  \\\\\n&= M_s\n\\end{aligned}\n\\]"
  }
]